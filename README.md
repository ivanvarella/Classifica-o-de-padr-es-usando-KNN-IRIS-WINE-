# AnÃ¡lise Comparativa de Algoritmos de ClassificaÃ§Ã£o: k-NN, Naive Bayes e Ãrvore de DecisÃ£o

[![Python](https://img.shields.io/badge/Python-3.12.7-blue.svg)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.5.0-green.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-Academic-red.svg)](https://opensource.org/licenses/)

## ğŸ“‹ Sobre o Projeto

Este projeto apresenta uma **anÃ¡lise comparativa abrangente** de trÃªs algoritmos clÃ¡ssicos de aprendizado de mÃ¡quina supervisionado para classificaÃ§Ã£o: **k-Nearest Neighbors (k-NN)**, **Gaussian Naive Bayes** e **Decision Tree (Ãrvore de DecisÃ£o)**. O estudo foi desenvolvido como parte da disciplina PPGEP9002 - InteligÃªncia Computacional para Engenharia de ProduÃ§Ã£o da UFRN.

### ğŸ¯ Objetivos

- Comparar o desempenho de trÃªs algoritmos de classificaÃ§Ã£o em datasets canÃ´nicos
- Analisar o impacto de hiperparÃ¢metros na performance dos modelos
- Avaliar a influÃªncia de diferentes proporÃ§Ãµes de treinamento (60%, 70%, 80%)
- Investigar o efeito da normalizaÃ§Ã£o Z-score no desempenho
- Fornecer insights sobre trade-offs entre acurÃ¡cia, interpretabilidade e eficiÃªncia

### ğŸ“Š Datasets Utilizados

#### 1. **Iris Dataset** (UCI ML Repository - ID: 53)

- **DescriÃ§Ã£o**: Dataset clÃ¡ssico de classificaÃ§Ã£o de espÃ©cies de Ã­ris
- **Amostras**: 150 (50 por classe)
- **Features**: 4 (comprimento e largura de sÃ©palas e pÃ©talas)
- **Classes**: 3 (Iris-setosa, Iris-versicolor, Iris-virginica)
- **CaracterÃ­sticas**: Linearmente separÃ¡vel, bem balanceado

#### 2. **Wine Dataset** (UCI ML Repository - ID: 109)

- **DescriÃ§Ã£o**: AnÃ¡lise quÃ­mica de vinhos de diferentes cultivares
- **Amostras**: 178
- **Features**: 13 (conteÃºdo alcoÃ³lico, Ã¡cido mÃ¡lico, etc.)
- **Classes**: 3 (diferentes cultivares de uva)
- **CaracterÃ­sticas**: Maior dimensionalidade, mais complexo que Iris

## ğŸ› ï¸ Tecnologias Utilizadas

### **Linguagem de ProgramaÃ§Ã£o**

- **Python 3.12.7**: Linguagem principal para desenvolvimento e anÃ¡lise de dados

### **Bibliotecas de AnÃ¡lise de Dados**

- **Pandas 2.2.2**: ManipulaÃ§Ã£o e anÃ¡lise de dados estruturados
- **NumPy 1.26.4**: ComputaÃ§Ã£o numÃ©rica e operaÃ§Ãµes com arrays multidimensionais
- **scikit-learn 1.5.0**: Biblioteca principal para algoritmos de machine learning

### **VisualizaÃ§Ã£o**

- **Matplotlib 3.9.0**: CriaÃ§Ã£o de grÃ¡ficos e visualizaÃ§Ãµes estÃ¡ticas
- **Seaborn 0.13.2**: VisualizaÃ§Ãµes estatÃ­sticas avanÃ§adas e estilizadas

### **Ambiente de Desenvolvimento**

- **Jupyter Notebook**: Ambiente interativo para desenvolvimento e anÃ¡lise
- **ucimlrepo 0.0.3**: Acesso direto aos datasets do UCI Machine Learning Repository

### **Gerenciamento de DependÃªncias**

- **uv**: Gerenciador de pacotes Python moderno e rÃ¡pido

## ğŸ§  Algoritmos de ClassificaÃ§Ã£o Implementados

### 1. **k-Nearest Neighbors (k-NN)**

**Conceito**: Algoritmo nÃ£o-paramÃ©trico baseado em instÃ¢ncias que classifica amostras baseado na classe majoritÃ¡ria de seus k vizinhos mais prÃ³ximos.

**Funcionamento**:

- Armazena todo o conjunto de treinamento
- Para cada amostra de teste, calcula distÃ¢ncias para todas as amostras de treino
- Seleciona os k vizinhos mais prÃ³ximos
- Classifica baseado na classe majoritÃ¡ria entre os vizinhos

**HiperparÃ¢metros Testados**:

- `k âˆˆ {1, 3, 5, 7, 9}`: NÃºmero de vizinhos considerados
- `n_jobs = -1`: ParalelizaÃ§Ã£o para aceleraÃ§Ã£o

**Vantagens**:

- Simples de implementar e entender
- NÃ£o assume distribuiÃ§Ã£o especÃ­fica dos dados
- Funciona bem com dados nÃ£o-lineares

**Desvantagens**:

- Computacionalmente custoso para prediÃ§Ã£o
- SensÃ­vel Ã  escala das features
- Pode ser afetado por ruÃ­do

### 2. **Gaussian Naive Bayes**

**Conceito**: Classificador probabilÃ­stico baseado no Teorema de Bayes com suposiÃ§Ã£o de independÃªncia condicional entre features.

**Funcionamento**:

- Calcula probabilidades a posteriori usando o Teorema de Bayes
- Assume que features seguem distribuiÃ§Ã£o Gaussiana
- Aplica suposiÃ§Ã£o "ingÃªnua" de independÃªncia entre features
- Classifica baseado na maior probabilidade a posteriori

**FÃ³rmula Principal**:

```
P(Classe|Features) âˆ P(Classe) Ã— âˆ P(Feature_i|Classe)
```

**Vantagens**:

- Extremamente rÃ¡pido para treinamento e prediÃ§Ã£o
- Funciona bem com poucos dados
- Robusto a features irrelevantes

**Desvantagens**:

- SuposiÃ§Ã£o de independÃªncia raramente Ã© verdadeira
- Pode ter performance limitada em problemas complexos

### 3. **Decision Tree (Ãrvore de DecisÃ£o)**

**Conceito**: Algoritmo que cria um modelo de Ã¡rvore de decisÃµes atravÃ©s de divisÃµes recursivas do espaÃ§o de features.

**Funcionamento**:

- Particiona recursivamente o espaÃ§o de features
- Cada nÃ³ representa uma decisÃ£o baseada em uma feature
- Cada folha representa uma classe predita
- Usa critÃ©rios como Gini ou Entropia para divisÃµes

**HiperparÃ¢metros Testados**:

- `max_depth âˆˆ {3, 5, None}`: Profundidade mÃ¡xima da Ã¡rvore
- `random_state = 42`: Semente para reprodutibilidade

**Vantagens**:

- Altamente interpretÃ¡vel
- NÃ£o requer normalizaÃ§Ã£o
- Captura interaÃ§Ãµes nÃ£o-lineares
- Funciona com features categÃ³ricas e numÃ©ricas

**Desvantagens**:

- Propenso a overfitting
- SensÃ­vel a pequenas mudanÃ§as nos dados
- Pode criar Ã¡rvores muito complexas

## ğŸ“ˆ Metodologia Experimental

### **ConfiguraÃ§Ã£o dos Experimentos**

- **RepetiÃ§Ãµes**: 10 execuÃ§Ãµes por configuraÃ§Ã£o para robustez estatÃ­stica
- **DivisÃ£o dos Dados**: Stratified split para manter proporÃ§Ã£o das classes
- **ProporÃ§Ãµes de Treino**: 60%, 70% e 80%
- **PrÃ©-processamento**: NormalizaÃ§Ã£o Z-score aplicada aos dados
- **MÃ©tricas**: AcurÃ¡cia, PrecisÃ£o (macro), RevocaÃ§Ã£o (macro), F1-Score (macro)

### **Pipeline de AvaliaÃ§Ã£o**

1. DivisÃ£o estratificada dos dados
2. NormalizaÃ§Ã£o Z-score (treinada apenas nos dados de treino)
3. Treinamento do modelo
4. PrediÃ§Ã£o no conjunto de teste
5. CÃ¡lculo das mÃ©tricas de avaliaÃ§Ã£o
6. AgregaÃ§Ã£o estatÃ­stica (mÃ©dia Â± desvio padrÃ£o)

## ğŸ“Š Resultados Principais

### **Performance Geral por Dataset**

#### **Dataset Iris**

- **Melhor Modelo**: k-NN (k=5) e Ãrvore de DecisÃ£o (max_depth=5)
- **AcurÃ¡cia MÃ©dia**: ~97-98%
- **ObservaÃ§Ã£o**: Todos os modelos apresentaram excelente performance devido Ã  separabilidade linear

#### **Dataset Wine**

- **Melhor Modelo**: k-NN (k=3-7) e Ãrvore de DecisÃ£o (max_depth=5)
- **AcurÃ¡cia MÃ©dia**: ~94-96%
- **ObservaÃ§Ã£o**: Maior variabilidade devido Ã  complexidade do dataset

### **AnÃ¡lise de HiperparÃ¢metros**

#### **k-NN**

- **k=1**: Maior sensibilidade a ruÃ­do, menor estabilidade
- **k=3-7**: Performance Ã³tima e estÃ¡vel
- **k=9**: Ligeira degradaÃ§Ã£o em alguns casos

#### **Ãrvore de DecisÃ£o**

- **max_depth=3**: Modelo simples, bom para evitar overfitting
- **max_depth=5**: Balance ideal entre complexidade e generalizaÃ§Ã£o
- **max_depth=None**: Risco de overfitting, especialmente com poucos dados

### **Impacto da NormalizaÃ§Ã£o**

- **k-NN**: Beneficiou significativamente da normalizaÃ§Ã£o Z-score
- **Naive Bayes**: Pouco impacto (jÃ¡ assume distribuiÃ§Ã£o normal)
- **Ãrvore de DecisÃ£o**: NÃ£o afetada (algoritmo baseado em divisÃµes)

## ğŸš€ Como Executar o Projeto

### **PrÃ©-requisitos**

- Python 3.12.7 ou superior
- Gerenciador de pacotes `uv` (recomendado) ou `pip`

### **InstalaÃ§Ã£o no Windows**

```bash
# 1. Clone o repositÃ³rio
git clone <url-do-repositorio>
cd "Atividade - 24.09.25 - classificaÃ§Ã£o de padrÃµes usando KNN"

# 2. Instale o uv (se nÃ£o tiver)
pip install uv

# 3. Instale as dependÃªncias
uv pip install -r requirements.txt

# 4. Inicie o Jupyter Notebook
jupyter notebook main.ipynb
```

### **InstalaÃ§Ã£o no Linux/macOS**

```bash
# 1. Clone o repositÃ³rio
git clone <url-do-repositorio>
cd "Atividade - 24.09.25 - classificaÃ§Ã£o de padrÃµes usando KNN"

# 2. Instale o uv (se nÃ£o tiver)
pip install uv
# ou no macOS com Homebrew:
# brew install uv

# 3. Instale as dependÃªncias
uv pip install -r requirements.txt

# 4. Inicie o Jupyter Notebook
jupyter notebook main.ipynb
```

### **ExecuÃ§Ã£o Alternativa com pip**

```bash
# Se preferir usar pip ao invÃ©s de uv
pip install -r requirements.txt
jupyter notebook main.ipynb
```

### **Estrutura de ExecuÃ§Ã£o do Notebook**

1. **CÃ©lula 1**: InstalaÃ§Ã£o das dependÃªncias (opcional se jÃ¡ instaladas)
2. **CÃ©lula 2**: ImportaÃ§Ã£o das bibliotecas
3. **CÃ©lula 3**: Carregamento e inspeÃ§Ã£o dos datasets
4. **CÃ©lula 4**: DefiniÃ§Ã£o das funÃ§Ãµes auxiliares
5. **CÃ©lulas 5-8**: Experimentos com k-NN
6. **CÃ©lulas 9-11**: Experimentos com Naive Bayes
7. **CÃ©lulas 12-15**: Experimentos com Ãrvore de DecisÃ£o
8. **CÃ©lulas 16-17**: AnÃ¡lise comparativa final

## ğŸ“ Estrutura do Projeto

```
ğŸ“¦ Atividade - 24.09.25 - classificaÃ§Ã£o de padrÃµes usando KNN
â”œâ”€â”€ ğŸ“„ main.ipynb                    # Notebook principal com toda a anÃ¡lise
â”œâ”€â”€ ğŸ“„ requirements.txt              # DependÃªncias do projeto
â”œâ”€â”€ ğŸ“„ README.md                     # Este arquivo
â”œâ”€â”€ ğŸ“„ RelatÃ³rio_v3.md               # RelatÃ³rio detalhado em Markdown
â”œâ”€â”€ ğŸ“„ tree.txt                      # Estrutura de diretÃ³rios
â””â”€â”€ ğŸ“ data/                         # Resultados dos experimentos
    â”œâ”€â”€ ğŸ“„ knn_raw_results.csv       # Resultados brutos do k-NN
    â”œâ”€â”€ ğŸ“„ nb_raw_results.csv        # Resultados brutos do Naive Bayes
    â”œâ”€â”€ ğŸ“„ dt_raw_results.csv        # Resultados brutos da Ãrvore de DecisÃ£o
    â””â”€â”€ ğŸ“ imgs/                     # GrÃ¡ficos e visualizaÃ§Ãµes
        â”œâ”€â”€ ğŸ“„ comparativo_desempenho_todos_modelos_output.png
        â”œâ”€â”€ ğŸ“„ impacto_k_acuracia_knn_output.png
        â”œâ”€â”€ ğŸ“„ impacto_max_depth_acuracia_dt_output.png
        â”œâ”€â”€ ğŸ“„ matriz_confusao_best_dt_output.png
        â”œâ”€â”€ ğŸ“„ matriz_confusao_best_knn_by_dataset_output.png
        â””â”€â”€ ğŸ“„ matriz_confusao_best_nb_80porc_output.png

```

## ğŸ” Principais Descobertas

### **1. ImportÃ¢ncia da NormalizaÃ§Ã£o**

- O k-NN foi significativamente beneficiado pela normalizaÃ§Ã£o Z-score
- A normalizaÃ§Ã£o Ã© crucial para algoritmos baseados em distÃ¢ncias

### **2. Trade-offs entre Modelos**

- **k-NN**: Melhor acurÃ¡cia, mas computacionalmente custoso
- **Naive Bayes**: Excelente baseline, extremamente rÃ¡pido
- **Ãrvore de DecisÃ£o**: Balance ideal entre performance e interpretabilidade

### **3. Sensibilidade a HiperparÃ¢metros**

- k-NN mostrou sensibilidade moderada ao valor de k
- Ãrvore de DecisÃ£o apresentou maior variabilidade com max_depth=None
- Naive Bayes nÃ£o requer ajuste de hiperparÃ¢metros

### **4. Robustez EstatÃ­stica**

- 10 repetiÃ§Ãµes foram suficientes para obter resultados estatisticamente robustos
- Desvio padrÃ£o baixo indica modelos estÃ¡veis

## ğŸ“ ConclusÃµes

Este estudo demonstrou que **nÃ£o existe um algoritmo universalmente superior**. A escolha do modelo ideal depende de:

- **Objetivo do projeto**: MÃ¡xima acurÃ¡cia vs. interpretabilidade vs. velocidade
- **CaracterÃ­sticas dos dados**: Dimensionalidade, separabilidade, tamanho
- **Recursos computacionais**: Tempo de treinamento e prediÃ§Ã£o
- **Requisitos de interpretabilidade**: Necessidade de explicar decisÃµes

### **RecomendaÃ§Ãµes por CenÃ¡rio**

- **MÃ¡xima AcurÃ¡cia**: k-NN com k=5-7
- **Interpretabilidade**: Ãrvore de DecisÃ£o com max_depth=5
- **Velocidade**: Naive Bayes
- **Baseline RÃ¡pido**: Naive Bayes
- **Dados Complexos**: k-NN ou Ãrvore de DecisÃ£o

## ğŸ‘¨â€ğŸ’» Autor

**Ivan Pedro Varella Albuquerque**  
_Disciplina_: PPGEP9002 - InteligÃªncia Computacional para Engenharia de ProduÃ§Ã£o  
_Professor_: Jose Alfredo Ferreira Costa  
_InstituiÃ§Ã£o_: Universidade Federal do Rio Grande do Norte (UFRN)

## ğŸ“š ReferÃªncias

- [UCI Machine Learning Repository](https://archive.ics.uci.edu/)
- [scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Seaborn Documentation](https://seaborn.pydata.org/)

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins acadÃªmicos como parte do programa de mestrado em Engenharia de ProduÃ§Ã£o da UFRN.

---

_Ãšltima atualizaÃ§Ã£o: Setembro 2025_
